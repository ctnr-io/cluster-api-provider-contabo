apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: ${CLUSTER_NAME}
spec:
  clusterNetwork:
    services:
      cidrBlocks: ["10.96.0.0/12"]
    pods:
      cidrBlocks: ["192.168.0.0/16"]
    serviceDomain: "cluster.local"
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: ContaboCluster
    name: ${CLUSTER_NAME}
  controlPlaneRef:
    kind: KubeadmControlPlane
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    name: "${CLUSTER_NAME}-control-plane"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: ContaboCluster
metadata:
  name: ${CLUSTER_NAME}
spec:
  controlPlaneEndpoint:
    host: "${CONTROL_PLANE_ENDPOINT_HOST}"
    port: ${CONTROL_PLANE_ENDPOINT_PORT:=6443}
  privateNetwork:
    region: ${CONTABO_REGION:=EU}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: "${CLUSTER_NAME}-control-plane"
spec:
  replicas: ${CONTROL_PLANE_MACHINE_COUNT:=3}
  version: ${KUBERNETES_VERSION}
  machineTemplate:
    infrastructureRef:
      kind: ContaboMachineTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
      name: "${CLUSTER_NAME}-control-plane"
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        certSANs:
          # Add the service name as a SAN for TLS validation
          # This allows connecting to the API server using the service name
          - "${CLUSTER_NAME}-apiserver"
          - "${CLUSTER_NAME}-apiserver.${NAMESPACE}.svc"
          - "${CLUSTER_NAME}-apiserver.${NAMESPACE}.svc.cluster.local"
          # Also add the external hostname if configured
          - "${CONTROL_PLANE_ENDPOINT_HOST}"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: ContaboMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-control-plane"
spec:
  template:
    spec:
      instance:
        name: ${CONTROL_PLANE_MACHINE_NAME:=}
        productId: ${CONTROL_PLANE_MACHINE_TYPE:=V45}
        provisioningType: ${CONTROL_PLANE_PROVISIONING_TYPE:=ReuseOnly} 
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: "${CLUSTER_NAME}-md-0"
  annotations:
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \"${WORKER_MACHINE_COUNT_MIN:=1}\"
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \"${WORKER_MACHINE_COUNT_MAX:=10}\"
spec:
  clusterName: "${CLUSTER_NAME}"
  replicas: ${WORKER_MACHINE_COUNT:=3}
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
      pool: worker-pool-0
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
        pool: worker-pool-0
    spec:
      clusterName: ${CLUSTER_NAME}
      version: ${KUBERNETES_VERSION}
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: "${CLUSTER_NAME}-md-0"
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: ContaboMachineTemplate
        name: "${CLUSTER_NAME}-md-0"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: ContaboMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
spec:
  template:
    spec:
      instance:
        productId: ${WORKER_MACHINE_TYPE:=V45}
        provisioningType: ${WORKER_PROVISIONING_TYPE:=ReuseOnly}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          name: '{{ ds.meta_data.hostname }}'
          kubeletExtraArgs:
            cloud-provider: external
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: "${CLUSTER_NAME}-worker-health-check"
spec:
  clusterName: "${CLUSTER_NAME}"
  maxUnhealthy: 40%
  nodeStartupTimeout: 10m
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
      pool: worker-pool-0
  unhealthyConditions:
    - type: Ready
      status: Unknown
      timeout: 5m
    - type: Ready
      status: "False"
      timeout: 5m
---
# Cluster Autoscaler Helm Addon
# Automatically installed via CAPI Helm addon provider
apiVersion: addons.cluster.x-k8s.io/v1alpha1
kind: HelmChartProxy
metadata:
  name: cluster-autoscaler
spec:
  clusterSelector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
  repoURL: https://kubernetes.github.io/autoscaler
  chartName: cluster-autoscaler
  version: ${CLUSTER_AUTOSCALER_VERSION:=9.29.3}
  namespace: kube-system
  releaseName: cluster-autoscaler
  valuesTemplate: |
    autoDiscovery:
      clusterName: {{ .Cluster.metadata.name }}
    
    cloudProvider: clusterapi
    clusterAPIMode: kubeconfig-kubeconfig
    clusterAPIKubeconfigSecret: cluster-autoscaler-kubeconfig
    clusterAPICloudConfigPath: /etc/kubernetes/mgmt-kubeconfig
    
    extraArgs:
      balance-similar-node-groups: true
      scale-down-enabled: true
      scale-down-delay-after-add: 1m
      scale-down-unneeded-time: 1m
      scale-down-utilization-threshold: 0.5
      skip-nodes-with-local-storage: false
      skip-nodes-with-system-pods: false
      max-node-provision-time: 15m
      v: 4
    
    resources:
      limits:
        cpu: 100m
        memory: 300Mi
      requests:
        cpu: 100m
        memory: 300Mi
    
    rbac:
      create: true
      serviceAccount:
        create: true
        name: cluster-autoscaler
---
# Secret containing management cluster kubeconfig
# This will be created by CAPI Helm addon and contains credentials to scale MachineDeployments
apiVersion: v1
kind: Secret
metadata:
  name: cluster-autoscaler-kubeconfig
  namespace: ${CLUSTER_NAMESPACE:=default}
type: addons.cluster.x-k8s.io/resource-set
stringData:
  value: |
    apiVersion: v1
    kind: Secret
    metadata:
      name: cluster-autoscaler-kubeconfig
      namespace: kube-system
    type: Opaque
    stringData:
      kubeconfig: |
        {{ .BootstrapKubeconfig | indent 8 }}