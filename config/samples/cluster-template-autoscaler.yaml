apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: ${CLUSTER_NAME}
spec:
  clusterNetwork:
    services:
      cidrBlocks: ["10.96.0.0/12"]
    pods:
      cidrBlocks: ["192.168.0.0/16"]
    serviceDomain: "cluster.local"
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: ContaboCluster
    name: ${CLUSTER_NAME}
  controlPlaneRef:
    kind: KubeadmControlPlane
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    name: "${CLUSTER_NAME}-control-plane"
---
apiVersion: v1
kind: Secret
metadata:
  name: ${CLUSTER_NAME}-encryption-key
  namespace: ${NAMESPACE:=default}
type: Opaque
stringData:
  encryption-config: |
    apiVersion: apiserver.config.k8s.io/v1
    kind: EncryptionConfiguration
    resources:
      - resources:
          - secrets
        providers:
          - secretbox:
              keys:
                - name: secretbox-key-0
                  secret: ${ENCRYPTION_KEY}
          - identity: {}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: ContaboCluster
metadata:
  name: ${CLUSTER_NAME}
spec:
  clusterUUID: ${CLUSTER_UUID:=} 
  controlPlaneEndpoint:
    host: "${CONTROL_PLANE_ENDPOINT_HOST}"
    port: ${CONTROL_PLANE_ENDPOINT_PORT:=6443}
  privateNetwork:
    region: ${CONTABO_REGION:=EU}
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: "${CLUSTER_NAME}-control-plane"
spec:
  replicas: ${CONTROL_PLANE_MACHINE_COUNT:=3}
  version: ${KUBERNETES_VERSION}
  machineTemplate:
    infrastructureRef:
      kind: ContaboMachineTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
      name: "${CLUSTER_NAME}-control-plane"
  kubeadmConfigSpec:
    initConfiguration:
      skipPhases:
      - addon/kube-proxy
    clusterConfiguration:
      apiServer:
        certSANs:
          # Add the service name as a SAN for TLS validation
          # This allows connecting to the API server using the service name
          - "${CLUSTER_NAME}-apiserver"
          - "${CLUSTER_NAME}-apiserver.${NAMESPACE}.svc"
          - "${CLUSTER_NAME}-apiserver.${NAMESPACE}.svc.cluster.local"
          # Also add the external hostname if configured
          - "${CONTROL_PLANE_ENDPOINT_HOST}"
        extraArgs:
          encryption-provider-config: /etc/kubernetes/encryption-config.yaml
        extraVolumes:
          - name: encryption-config
            hostPath: /etc/kubernetes/encryption-config.yaml
            mountPath: /etc/kubernetes/encryption-config.yaml
            readOnly: true
            pathType: File
    files:
      - path: /etc/kubernetes/encryption-config.yaml
        owner: root:root
        permissions: "0600"
        contentFrom:
          secret:
            name: ${CLUSTER_NAME}-encryption-key
            key: encryption-config
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: ContaboMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-control-plane"
spec:
  template:
    spec:
      instance:
        name: ${CONTROL_PLANE_MACHINE_NAME:=}
        productId: ${CONTROL_PLANE_MACHINE_TYPE:=V45}
        provisioningType: ${CONTROL_PLANE_PROVISIONING_TYPE:=ReuseOnly} 
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: "${CLUSTER_NAME}-md-0"
  annotations:
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \"${WORKER_MACHINE_COUNT_MIN:=1}\"
    cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \"${WORKER_MACHINE_COUNT_MAX:=10}\"
spec:
  clusterName: "${CLUSTER_NAME}"
  replicas: ${WORKER_MACHINE_COUNT:=3}
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
      pool: worker-pool-0
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
        pool: worker-pool-0
    spec:
      clusterName: ${CLUSTER_NAME}
      version: ${KUBERNETES_VERSION}
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: "${CLUSTER_NAME}-md-0"
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: ContaboMachineTemplate
        name: "${CLUSTER_NAME}-md-0"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: ContaboMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
spec:
  template:
    spec:
      instance:
        productId: ${WORKER_MACHINE_TYPE:=V45}
        provisioningType: ${WORKER_PROVISIONING_TYPE:=ReuseOnly}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: "${CLUSTER_NAME}-worker-health-check"
spec:
  clusterName: "${CLUSTER_NAME}"
  maxUnhealthy: 40%
  nodeStartupTimeout: 10m
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
      pool: worker-pool-0
  unhealthyConditions:
    - type: Ready
      status: Unknown
      timeout: 5m
    - type: Ready
      status: "False"
      timeout: 5m
---
# Cluster Autoscaler Helm Addon
# Automatically installed via CAPI Helm addon provider
apiVersion: addons.cluster.x-k8s.io/v1alpha1
kind: HelmChartProxy
metadata:
  name: ${CLUSTER_NAME}-autoscaler
spec:
  clusterSelector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
  repoURL: https://kubernetes.github.io/autoscaler
  chartName: cluster-autoscaler
  version: ${CLUSTER_AUTOSCALER_VERSION:=9.52.1}
  namespace: kube-system
  releaseName: cluster-autoscaler
  valuesTemplate: |
    autoDiscovery:
      clusterName: ${CLUSTER_NAME}
    
    cloudProvider: clusterapi
    # Running in management cluster, use in-cluster config for CAPI resources
    # Connect to workload cluster via kubeconfig secret
    clusterAPIMode: kubeconfig-incluster
    clusterAPIKubeconfigSecret: ${CLUSTER_NAME}-kubeconfig
    
    extraArgs:
      balance-similar-node-groups: true
      scale-down-enabled: false
      scale-down-delay-after-add: 1m
      scale-down-unneeded-time: 1m
      scale-down-utilization-threshold: 0.5
      skip-nodes-with-local-storage: false
      skip-nodes-with-system-pods: false
      max-node-provision-time: 15m
      v: 4
    
    resources:
      limits:
        cpu: 100m
        memory: 300Mi
      requests:
        cpu: 100m
        memory: 300Mi
    
    rbac:
      create: true
      clusterScoped: true
      serviceAccount:
        create: true
        name: cluster-autoscaler
